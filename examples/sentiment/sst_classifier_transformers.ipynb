{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.6.1 in /home/jupyter/.local/lib/python3.7/site-packages (4.6.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.6.1) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.6.1) (4.8.1)\n",
      "Requirement already satisfied: sacremoses in /home/jupyter/.local/lib/python3.7/site-packages (from transformers==4.6.1) (0.0.46)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.6.1) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.6.1) (2021.8.28)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.6.1) (4.62.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/jupyter/.local/lib/python3.7/site-packages (from transformers==4.6.1) (0.10.3)\n",
      "Requirement already satisfied: filelock in /home/jupyter/.local/lib/python3.7/site-packages (from transformers==4.6.1) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /home/jupyter/.local/lib/python3.7/site-packages (from transformers==4.6.1) (0.0.8)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.6.1) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.6.1) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.6.1) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.6.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.6.1) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.6.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.6.1) (2.10)\n",
      "Requirement already satisfied: click in /home/jupyter/.local/lib/python3.7/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.6.1) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  274k  100  274k    0     0   804k      0 --:--:-- --:--:-- --:--:--  801k\n"
     ]
    }
   ],
   "source": [
    "!curl https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt --output dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2109k  100 2109k    0     0  4646k      0 --:--:-- --:--:-- --:--:-- 4646k\n"
     ]
    }
   ],
   "source": [
    "!curl https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt --output train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcdf59044ac4abc8a12ccbd5d6cfa6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3074052201455f9bb4a3ea4847bc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742c7f5264d44e5cbef1b420d988abbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e3636e182d4d979224bfe8d101da77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self.linear = nn.Linear(self.bert_model.config.hidden_size, num_labels)\n",
    "\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, label=None):\n",
    "        bert_out = self.bert_model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          token_type_ids=token_type_ids)\n",
    "        \n",
    "        logits = self.linear(bert_out.pooler_output)\n",
    "\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = self.loss_function(logits, label)\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode('The best movie ever!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1109, 1436, 2523, 1518, 106, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] The best movie ever! [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "result = tokenizer(\n",
    "    ['The best movie ever!', 'Aweful movie'],\n",
    "    max_length=10,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1109, 1436, 2523, 1518,  106,  102,    0,    0,    0],\n",
       "        [ 101,  138, 7921, 2365, 2523,  102,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1109, 1436, 2523, 1518,  106,  102,    0,    0,    0],\n",
       "        [ 101,  138, 7921, 2365, 2523,  102,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file_path, batch_size, tokenizer, max_length):\n",
    "    batches = []\n",
    "    with open(file_path) as f:\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            text = line.strip()\n",
    "            label = int(text[1])\n",
    "            text = re.sub('\\)+', '', re.sub('\\(\\d ', '', text))\n",
    "            text = text.replace('-LRB-', '(').replace('-RRB-', ')')\n",
    "            \n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "\n",
    "            if len(texts) == batch_size:\n",
    "                batch = tokenizer(\n",
    "                    texts,\n",
    "                    max_length=max_length,\n",
    "                    pad_to_max_length=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt')\n",
    "                batch['label'] = torch.tensor(labels)\n",
    "                batches.append(batch)\n",
    "                \n",
    "                texts = []\n",
    "                labels = []\n",
    "        \n",
    "        if texts:\n",
    "            batch = tokenizer(\n",
    "                texts,\n",
    "                max_length=max_length,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt')\n",
    "            batch['label'] = torch.tensor(labels)\n",
    "            batches.append(batch)\n",
    "\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_dataset('train.txt', batch_size=32, tokenizer=tokenizer, max_length=128)\n",
    "dev_data = read_dataset('dev.txt', batch_size=32, tokenizer=tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267, 35)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to(batch, device):\n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abfb5f1aa614a8d951a2b134448140e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(model_name=BERT_MODEL, num_labels=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.7918, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " tensor([[-1.3490e-01, -2.0231e-01,  1.0244e+00, -1.2546e-01,  5.7981e-02],\n",
       "         [-2.0607e-02, -3.4415e-01,  9.9455e-01, -2.5073e-01, -1.3179e-02],\n",
       "         [-1.9296e-02, -3.8154e-01,  1.0262e+00, -2.9049e-01, -6.5379e-02],\n",
       "         [-1.4048e-01, -1.7977e-01,  1.0589e+00, -1.6484e-01,  8.2360e-02],\n",
       "         [-4.8007e-02, -1.9766e-01,  9.7215e-01, -1.9844e-01,  1.0912e-01],\n",
       "         [-8.7912e-02, -1.2186e-01,  1.0786e+00, -1.2343e-01,  2.1999e-02],\n",
       "         [-3.2315e-02, -2.4036e-01,  1.0648e+00, -2.6812e-01, -9.6161e-03],\n",
       "         [-9.7170e-02, -2.2085e-01,  9.9116e-01, -1.7828e-01,  4.6678e-02],\n",
       "         [-1.3612e-01, -2.1868e-01,  9.5757e-01, -1.4523e-01,  1.0221e-01],\n",
       "         [ 1.0864e-02, -3.7965e-01,  9.8214e-01, -2.5621e-01, -2.0941e-02],\n",
       "         [-8.7822e-02, -2.9356e-01,  9.3817e-01, -1.9523e-01,  1.0290e-01],\n",
       "         [-1.1786e-02, -3.7825e-01,  9.4710e-01, -2.7533e-01, -4.2856e-02],\n",
       "         [-1.0005e-01, -1.8786e-01,  1.0048e+00, -1.9202e-01,  8.7250e-02],\n",
       "         [-3.9071e-02, -3.4351e-01,  9.6906e-01, -2.3472e-01,  2.6486e-02],\n",
       "         [-7.6818e-02, -1.9394e-01,  1.0820e+00, -1.8544e-01,  4.8468e-02],\n",
       "         [-1.6455e-01, -2.6699e-01,  1.0140e+00, -2.2390e-01,  1.1493e-01],\n",
       "         [-4.5521e-03, -3.0967e-01,  9.5417e-01, -2.1848e-01, -3.3861e-02],\n",
       "         [-1.8017e-01, -1.1369e-01,  9.5452e-01, -1.5151e-01,  1.2940e-01],\n",
       "         [-1.2456e-01, -2.0924e-01,  9.4983e-01, -1.6109e-01,  1.0802e-01],\n",
       "         [-8.1523e-02, -3.0060e-01,  9.8976e-01, -1.9863e-01,  9.3529e-04],\n",
       "         [-1.1089e-01, -2.4305e-01,  1.0987e+00, -1.5743e-01,  5.4592e-02],\n",
       "         [-9.4608e-02, -2.7250e-01,  9.9412e-01, -2.0709e-01,  9.5233e-02],\n",
       "         [-6.3994e-02, -2.9232e-01,  9.5214e-01, -2.3132e-01,  4.1793e-02],\n",
       "         [-1.2384e-01, -2.5892e-01,  1.0313e+00, -1.0592e-01,  5.8213e-02],\n",
       "         [-1.5783e-01, -1.5776e-01,  1.0254e+00, -1.9040e-01,  9.4582e-02],\n",
       "         [-9.1645e-02, -2.3147e-01,  1.0620e+00, -1.3797e-01,  4.6754e-02],\n",
       "         [-1.3927e-01, -2.2918e-01,  1.0430e+00, -1.4190e-01,  5.7379e-02],\n",
       "         [-6.6991e-02, -2.7825e-01,  1.1125e+00, -2.4807e-01,  3.9614e-02],\n",
       "         [-1.1839e-01, -2.0717e-01,  1.0237e+00, -2.3082e-01,  1.3976e-01],\n",
       "         [-1.4361e-01, -2.3965e-01,  1.0348e+00, -1.5140e-01,  8.5341e-02],\n",
       "         [-1.3302e-01, -1.8033e-01,  9.8485e-01, -1.3570e-01,  7.6460e-02],\n",
       "         [-1.4665e-01, -2.3362e-01,  9.4259e-01, -1.9133e-01,  1.0601e-01]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_to(dev_data[0], device)\n",
    "model(**dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=1000,\n",
    "    num_training_steps=len(train_data) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "train loss = 1.6137793064117432, accuracy = 0.24367977528089887\n",
      "dev loss = 1.660991907119751, accuracy = 0.259763851044505\n",
      "epoch = 1\n",
      "train loss = 1.4579780101776123, accuracy = 0.36739232209737827\n",
      "dev loss = 1.6124693155288696, accuracy = 0.2851952770208901\n",
      "epoch = 2\n",
      "train loss = 1.2845360040664673, accuracy = 0.43469101123595505\n",
      "dev loss = 1.3206464052200317, accuracy = 0.4150772025431426\n",
      "epoch = 3\n",
      "train loss = 1.0848054885864258, accuracy = 0.5149812734082397\n",
      "dev loss = 1.3720569610595703, accuracy = 0.4223433242506812\n",
      "epoch = 4\n",
      "train loss = 0.9170249700546265, accuracy = 0.6067415730337079\n",
      "dev loss = 1.2367522716522217, accuracy = 0.49227974568574023\n",
      "epoch = 5\n",
      "train loss = 0.7437633275985718, accuracy = 0.7036516853932584\n",
      "dev loss = 1.3211069107055664, accuracy = 0.5131698455949137\n",
      "epoch = 6\n",
      "train loss = 0.5801069140434265, accuracy = 0.7814840823970037\n",
      "dev loss = 1.469807744026184, accuracy = 0.4895549500454133\n",
      "epoch = 7\n",
      "train loss = 0.4505126178264618, accuracy = 0.8398876404494382\n",
      "dev loss = 1.5698213577270508, accuracy = 0.49227974568574023\n",
      "epoch = 8\n",
      "train loss = 0.36554816365242004, accuracy = 0.8725421348314607\n",
      "dev loss = 1.757430911064148, accuracy = 0.4713896457765668\n",
      "epoch = 9\n",
      "train loss = 0.3254665434360504, accuracy = 0.8843632958801498\n",
      "dev loss = 1.768052101135254, accuracy = 0.485921889191644\n",
      "epoch = 10\n",
      "train loss = 0.2793257534503937, accuracy = 0.9036750936329588\n",
      "dev loss = 2.075148344039917, accuracy = 0.44595821980018163\n",
      "epoch = 11\n",
      "train loss = 0.23429811000823975, accuracy = 0.9178370786516854\n",
      "dev loss = 2.448943853378296, accuracy = 0.4069028156221617\n",
      "epoch = 12\n",
      "train loss = 0.20743674039840698, accuracy = 0.9307116104868914\n",
      "dev loss = 2.3763768672943115, accuracy = 0.4296094459582198\n",
      "epoch = 13\n",
      "train loss = 0.19929319620132446, accuracy = 0.9294241573033708\n",
      "dev loss = 2.2876923084259033, accuracy = 0.4359673024523161\n",
      "epoch = 14\n",
      "train loss = 0.1755814105272293, accuracy = 0.9421816479400749\n",
      "dev loss = 2.2221181392669678, accuracy = 0.4623069936421435\n",
      "epoch = 15\n",
      "train loss = 0.14220254123210907, accuracy = 0.954119850187266\n",
      "dev loss = 2.3619368076324463, accuracy = 0.45322434150772023\n",
      "epoch = 16\n",
      "train loss = 0.1156792938709259, accuracy = 0.9618445692883895\n",
      "dev loss = 2.435628890991211, accuracy = 0.45049954586739327\n",
      "epoch = 17\n",
      "train loss = 0.10982456058263779, accuracy = 0.9640683520599251\n",
      "dev loss = 2.5141472816467285, accuracy = 0.45958219800181654\n",
      "epoch = 18\n",
      "train loss = 0.10740718990564346, accuracy = 0.9638342696629213\n",
      "dev loss = 2.5563888549804688, accuracy = 0.4550408719346049\n",
      "epoch = 19\n",
      "train loss = 0.10073256492614746, accuracy = 0.96687734082397\n",
      "dev loss = 2.5911753177642822, accuracy = 0.44504995458673935\n",
      "epoch = 20\n",
      "train loss = 0.10796570777893066, accuracy = 0.9657069288389513\n",
      "dev loss = 2.620990753173828, accuracy = 0.4514078110808356\n",
      "epoch = 21\n",
      "train loss = 0.10247441381216049, accuracy = 0.9654728464419475\n",
      "dev loss = 2.5908944606781006, accuracy = 0.4550408719346049\n",
      "epoch = 22\n",
      "train loss = 0.1177363470196724, accuracy = 0.9607911985018727\n",
      "dev loss = 2.5422143936157227, accuracy = 0.4641235240690282\n",
      "epoch = 23\n",
      "train loss = 0.1281673014163971, accuracy = 0.9556413857677902\n",
      "dev loss = 2.4650444984436035, accuracy = 0.47411444141689374\n",
      "epoch = 24\n",
      "train loss = 0.14524538815021515, accuracy = 0.9489700374531835\n",
      "dev loss = 2.3846662044525146, accuracy = 0.48319709355131696\n",
      "epoch = 25\n",
      "train loss = 0.16040660440921783, accuracy = 0.9418305243445693\n",
      "dev loss = 2.2974600791931152, accuracy = 0.49954586739327883\n",
      "epoch = 26\n",
      "train loss = 0.16472601890563965, accuracy = 0.9421816479400749\n",
      "dev loss = 2.274955987930298, accuracy = 0.5013623978201635\n",
      "epoch = 27\n",
      "train loss = 0.1454993188381195, accuracy = 0.946746254681648\n",
      "dev loss = 2.2845966815948486, accuracy = 0.4940962761126249\n",
      "epoch = 28\n",
      "train loss = 0.1216745600104332, accuracy = 0.9583333333333334\n",
      "dev loss = 2.292205333709717, accuracy = 0.49318801089918257\n",
      "epoch = 29\n",
      "train loss = 0.13457681238651276, accuracy = 0.9544709737827716\n",
      "dev loss = 2.294158697128296, accuracy = 0.4913714804722979\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'epoch = {epoch}')\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    total_instances = 0\n",
    "    correct_instances = 0\n",
    "    for batch in train_data:\n",
    "        batch_size = batch['input_ids'].size(0)\n",
    "        move_to(batch, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, logits = model(**batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "        losses.append(loss)\n",
    "        \n",
    "        total_instances += batch_size\n",
    "        correct_instances += torch.sum(torch.argmax(logits, dim=-1) == batch['label']).item()\n",
    "    \n",
    "    avr_loss = sum(losses) / len(losses)\n",
    "    accuracy = correct_instances / total_instances\n",
    "    print(f'train loss = {avr_loss}, accuracy = {accuracy}')\n",
    "    \n",
    "    losses = []\n",
    "    total_instances = 0\n",
    "    correct_instances = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for batch in dev_data:\n",
    "        batch_size = batch['input_ids'].size(0)\n",
    "        move_to(batch, device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(**batch)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        total_instances += batch_size\n",
    "        correct_instances += torch.sum(torch.argmax(logits, dim=-1) == batch['label']).item()\n",
    "\n",
    "    avr_loss = sum(losses) / len(losses)\n",
    "    accuracy = correct_instances / total_instances\n",
    "    \n",
    "    print(f'dev loss = {avr_loss}, accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "managed-notebooks.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/managed-notebooks:m80"
  },
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
